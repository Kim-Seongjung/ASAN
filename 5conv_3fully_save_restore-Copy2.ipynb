{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n",
      "100 100\n"
     ]
    }
   ],
   "source": [
    "#conv Neural Network\n",
    "# tensorboard --logdir=/home/ncc/notebook/learn/tensorboard/log\n",
    "\"\"\"\n",
    "created by kim Seong jung\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os \n",
    "\n",
    "file_locate='../Training_data_set/'\n",
    "sess = tf.InteractiveSession()\n",
    "aug_flag=False\n",
    "model_number=1\n",
    "if aug_flag == True :\n",
    "\n",
    "    img_row = 64\n",
    "    img_col = 64\n",
    "    in_ch =1\n",
    "else:\n",
    "    img_row = 100\n",
    "    img_col = 100\n",
    "    in_ch =1\n",
    "    \n",
    "divide_flag= False\n",
    "restore_flag =False\n",
    "save_flag=True\n",
    "learning_rate=1e-4\n",
    "\n",
    "model_save_path='/home/user01/ASAN/Model_save/'+str(model_number)+'/'\n",
    "if os.path.isdir(model_save_path)==False:\n",
    "    print 'a'\n",
    "    os.mkdir(model_save_path)\n",
    "if restore_flag ==True:\n",
    "    restore_path=model_save_path\n",
    "batch_size=30\n",
    "print img_row ,img_col\n",
    "n_classes =2\n",
    "\n",
    "out_ch1=200\n",
    "out_ch2=200\n",
    "out_ch3=200\n",
    "out_ch4=200\n",
    "out_ch5=200\n",
    "\n",
    "\n",
    "fully_ch1=1024\n",
    "fully_ch2 =1024\n",
    "fully_ch3 =1024\n",
    "\n",
    "\n",
    "\n",
    "strides_1=[1,2,2,1]\n",
    "strides_2=[1,1,1,1]\n",
    "strides_3=[1,1,1,1]\n",
    "strides_4=[1,1,1,1]\n",
    "strides_5=[1,1,1,1]\n",
    "\n",
    "with tf.device('/gpu:3'):\n",
    "\n",
    "    x= tf.placeholder(\"float\",shape=[None,img_col , img_row , in_ch],  name = 'x-input')\n",
    "    y_=tf.placeholder(\"float\",shape=[None , n_classes] , name = 'y-input')\n",
    "    keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "    x_image= tf.reshape(x,[-1,img_row,img_col,in_ch])\n",
    "\n",
    "iterate=100000\n",
    "\n",
    "\n",
    "\n",
    "weight_row =3 ; weight_col=3\n",
    "\n",
    "\n",
    "pooling_row_size1=int(img_row/2)\n",
    "pooling_row_size2=int(pooling_row_size1/2)\n",
    "pooling_row_size3=int(pooling_row_size2/2)\n",
    "pooling_row_size4=int(pooling_row_size3/2)\n",
    "pooling_row_size5=int(pooling_row_size4/2)\n",
    "pooling_col_size1=int(img_col/2)\n",
    "pooling_col_size2=int(pooling_col_size1/2)\n",
    "pooling_col_size3=int(pooling_col_size2/2)\n",
    "pooling_col_size4=int(pooling_col_size3/2)\n",
    "pooling_col_size5=int(pooling_col_size4/2)\n",
    "\n",
    "print img_col , img_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore Weight and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/user01/ASAN/ASAN'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data (310, 100, 100, 1)\n",
      "Training Data Label (310, 2)\n",
      "Test Data Label (39, 2)\n",
      "val Data Label (39, 100, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:3'):\n",
    "#with tf.device('/gpu:3'):\n",
    "    if aug_flag == True:\n",
    "        img=np.load(file_locate+'train_img_'+str(model_number)+'.npy');\n",
    "        lab=np.load(file_locate+'train_lab_'+str(model_number)+'.npy');\n",
    "\n",
    "        train_img=img[:int(len(img)*0.8)]\n",
    "        train_lab=lab[:int(len(lab)*0.8)]\n",
    "        \n",
    "        val_img=img[int(len(img)*0.8):int(len(img)*0.9)]\n",
    "        val_lab=lab[int(len(lab)*0.8):int(len(img)*0.9)]\n",
    "        \n",
    "        \n",
    "        test_img=img[int(len(img)*0.9):]\n",
    "        test_lab=lab[int(len(lab)*0.9):]\n",
    "    else:   \n",
    "        if divide_flag == False:\n",
    "            img=np.load(file_locate+'train_img_'+str(model_number)+'.npy');\n",
    "            lab=np.load(file_locate+'train_lab_'+str(model_number)+'.npy');\n",
    "\n",
    "            train_img=img[:int(len(img)*0.8)]\n",
    "            train_lab=lab[:int(len(lab)*0.8)]\n",
    "\n",
    "            val_img=img[int(len(img)*0.8):int(len(img)*0.9)]\n",
    "            val_lab=lab[int(len(lab)*0.8):int(len(img)*0.9)]\n",
    "\n",
    "\n",
    "            test_img=img[int(len(img)*0.9):]\n",
    "            test_lab=lab[int(len(lab)*0.9):]\n",
    "            print \"Training Data\",np.shape(train_img)\n",
    "            print \"Training Data Label\",np.shape(train_lab)\n",
    "            print \"Test Data Label\",np.shape(test_lab)\n",
    "            print \"val Data Label\" , np.shape(val_img)\n",
    "\n",
    "            n_train= np.shape(train_img)[0]\n",
    "            n_train_lab = np.shape(train_lab)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"def weight_variable(name,shape):\n",
    "    #initial = tf.truncated_normal(shape , stddev=0.1)\n",
    "    initial = tf.get_variable(name,shape=shape , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    return tf.Variable(initial)\"\"\"\n",
    "with tf.device('/gpu:3'):\n",
    "    def bias_variable(shape):\n",
    "        initial = tf.constant(0.1 , shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:3'):\n",
    "    def next_batch(batch_size , image , label):\n",
    "\n",
    "        a=np.random.randint(np.shape(image)[0] -batch_size)\n",
    "        batch_x = image[a:a+batch_size,:]\n",
    "        batch_y= label[a:a+batch_size,:]\n",
    "        return batch_x, batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:3'):\n",
    "\n",
    "    def conv2d(x,w,strides_):\n",
    "        return tf.nn.conv2d(x,w, strides = strides_, padding='SAME')\n",
    "    def max_pool_2x2(x):\n",
    "        return tf.nn.max_pool(x , ksize=[1,2,2,1] ,strides = [1,2,2,1] , padding = 'SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "with tf.device('/gpu:3'):\n",
    "    if restore_flag==False:\n",
    "        with tf.variable_scope(\"layer1\") as scope:\n",
    "            try:\n",
    "                w_conv1 = tf.get_variable(\"W1\",[weight_row,weight_col,in_ch,out_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                w_conv1 = tf.get_variable(\"W1\",[weight_row,weight_col,in_ch,out_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "        with tf.variable_scope(\"layer1\") as scope:\n",
    "            try:\n",
    "                b_conv1 = bias_variable([out_ch1])\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                b_conv1 = bias_variable([out_ch1])\n",
    "        with tf.variable_scope('layer2') as scope:\n",
    "            try:\n",
    "                w_conv2 = tf.get_variable(\"W2\",[weight_row,weight_col,out_ch1,out_ch2] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                w_conv2 = tf.get_variable(\"W2\",[weight_row,weight_col,out_ch1,out_ch2] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        with tf.variable_scope('layer2') as scope:\n",
    "            try:\n",
    "                b_conv2= bias_variable([out_ch2])\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                b_conv2= bias_variable([out_ch2])\n",
    "\n",
    "        with tf.variable_scope('layer3') as scope:\n",
    "            try:\n",
    "                w_conv3 = tf.get_variable(\"W3\" ,[weight_row,weight_col,out_ch2,out_ch3] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                w_conv3 = tf.get_variable(\"W3\" ,[weight_row,weight_col,out_ch2,out_ch3] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "        with tf.variable_scope('layer3') as scope:\n",
    "            try:\n",
    "                b_conv3 = bias_variable([out_ch3])\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                b_conv3 = bias_variable([out_ch3])\n",
    "\n",
    "        with tf.variable_scope('layer4') as scope:\n",
    "            try:\n",
    "                w_conv4 =tf.get_variable(\"W4\" ,[weight_row,weight_col,out_ch3,out_ch4] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                w_conv3 = tf.get_variable(\"W4\" ,[weight_row,weight_col,out_ch3,out_ch4] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "        with tf.variable_scope('layer4') as scope:\n",
    "            try:\n",
    "                b_conv4 = bias_variable([out_ch4])\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                b_conv3 = bias_variable([out_ch4])\n",
    "\n",
    "        with tf.variable_scope('layer5') as scope:\n",
    "            try:\n",
    "                w_conv5 = tf.get_variable(\"W5\",[weight_row,weight_col,out_ch4,out_ch5] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                w_conv3 = tf.get_variable(\"W5\" ,[weight_row,weight_col,out_ch4,out_ch5] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "        with tf.variable_scope('layer5') as scope:\n",
    "            try:\n",
    "                b_conv5 = bias_variable([out_ch5])\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                b_conv3 = bias_variable([out_ch5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "with tf.device('/gpu:3'):\n",
    "    if restore_flag==True:\n",
    "        with tf.variable_scope(\"layer1\") as scope:\n",
    "            try:\n",
    "                w_conv1 = tf.Variable(np.load(restore_path+'/w_conv1.npy'),name=\"W1\")\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                w_conv1 = tf.Variable(np.load(restore_path+'/w_conv1.npy'),name=\"W1\")\n",
    "        with tf.variable_scope(\"layer1\") as scope:\n",
    "            try:\n",
    "                b_conv1 = tf.Variable(np.load(restore_path+'/b_conv1.npy'),name=\"B1\")\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                b_conv1 =tf.Variable(np.load(restore_path+'/b_conv1.npy'),name=\"B1\")\n",
    "        with tf.variable_scope(\"layer2\") as scope:\n",
    "            try:\n",
    "                w_conv2 = tf.Variable(np.load(restore_path+'/w_conv2.npy'),name=\"W2\")\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                w_conv2 = tf.Variable(np.load(restore_path+'/w_conv2.npy'),name=\"W2\")\n",
    "        with tf.variable_scope(\"layer2\") as scope:\n",
    "            try:\n",
    "                b_conv2 = tf.Variable(np.load(restore_path+'/b_conv2.npy'),name=\"B2\")\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                b_conv2 =tf.Variable(np.load(restore_path+'/b_conv2.npy'),name=\"B2\")\n",
    "        with tf.variable_scope(\"layer3\") as scope:\n",
    "            try:\n",
    "                w_conv3 = tf.Variable(np.load(restore_path+'/w_conv3.npy'),name=\"W3\")\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                w_conv3 = tf.Variable(np.load(restore_path+'/w_conv3.npy'),name=\"W3\")\n",
    "        with tf.variable_scope(\"layer3\") as scope:\n",
    "            try:\n",
    "                b_conv3 = tf.Variable(np.load(restore_path+'/b_conv3.npy'),name=\"B3\")\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                b_conv3 =tf.Variable(np.load(restore_path+'/b_conv3.npy'),name=\"B3\")\n",
    "        with tf.variable_scope(\"layer4\") as scope:\n",
    "            try:\n",
    "                w_conv4 = tf.Variable(np.load(restore_path+'/w_conv4.npy'),name=\"W4\")\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                w_conv4 = tf.Variable(np.load(restore_path+'/w_conv4.npy'),name=\"W4\")\n",
    "        with tf.variable_scope(\"layer4\") as scope:\n",
    "            try:\n",
    "                b_conv4 = tf.Variable(np.load(restore_path+'/b_conv4.npy'),name=\"B4\")\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                b_conv4 =tf.Variable(np.load(restore_path+'/b_conv4.npy'),name=\"B4\")\n",
    "        with tf.variable_scope(\"layer5\") as scope:\n",
    "            try:\n",
    "                w_conv5 = tf.Variable(np.load(restore_path+'/w_conv5.npy'),name=\"W5\")\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                w_conv5 = tf.Variable(np.load(restore_path+'/w_conv5.npy'),name=\"W5\")\n",
    "        with tf.variable_scope(\"layer5\") as scope:\n",
    "            try:\n",
    "                b_conv5 = tf.Variable(np.load(restore_path+'/b_conv5.npy'),name=\"B5\")\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                b_conv5 =tf.Variable(np.load(restore_path+'/b_conv5.npy'),name=\"B5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Relu:0\", shape=(?, 50, 50, 200), dtype=float32, device=/device:GPU:3)\n",
      "Tensor(\"MaxPool:0\", shape=(?, 25, 25, 200), dtype=float32, device=/device:GPU:3)\n",
      "Tensor(\"Relu_2:0\", shape=(?, 25, 25, 200), dtype=float32, device=/device:GPU:3)\n",
      "Tensor(\"Relu_3:0\", shape=(?, 25, 25, 200), dtype=float32, device=/device:GPU:3)\n",
      "Tensor(\"MaxPool_2:0\", shape=(?, 13, 13, 200), dtype=float32, device=/device:GPU:3)\n"
     ]
    }
   ],
   "source": [
    "#conncect hidden layer \n",
    "with tf.device('/gpu:3'):\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image , w_conv1 ,strides_1)+b_conv1)\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_conv1 , w_conv2 ,strides_2)+b_conv2)\n",
    "    h_conv2 = max_pool_2x2(h_conv2)#pooling\n",
    "    \n",
    "    h_conv3 = tf.nn.relu(conv2d(h_conv2 , w_conv3,strides_3)+b_conv3)\n",
    "    h_conv4 = tf.nn.relu(conv2d(h_conv3 , w_conv4,strides_4)+b_conv4)\n",
    "    h_pool4 = max_pool_2x2(h_conv4) #pooling \n",
    "\n",
    "    h_conv5 = tf.nn.relu(conv2d(h_conv4, w_conv5,strides_5)+b_conv5)\n",
    "    h_conv5= max_pool_2x2(h_conv5) #pooling \n",
    "\n",
    "    print h_conv1\n",
    "    print h_conv2\n",
    "    print h_conv3\n",
    "    print h_conv4\n",
    "    print h_conv5\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:3'):\n",
    "    end_conv = h_conv5\n",
    "    #print conv2d(h_pool1 , w_conv2).get_shape()\n",
    "    end_conv_row=int(h_conv5.get_shape()[1])\n",
    "    end_conv_col=int(h_conv5.get_shape()[2])\n",
    "    end_conv_ch=int(h_conv5.get_shape()[3])\n",
    "    #connect fully connected layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#connect fully connected layer \n",
    "if restore_flag==False:\n",
    "    with tf.device('/gpu:3'):\n",
    "        with tf.variable_scope(\"fc1\") as scope:\n",
    "            try:\n",
    "                w_fc1=tf.get_variable(\"fc1_W\",[end_conv_col*end_conv_row*end_conv_ch,fully_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                w_fc1=tf.get_variable(\"fc1_W\",[end_conv_col*end_conv_row*end_conv_ch,fully_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "            try:\n",
    "                b_fc1 = bias_variable([fully_ch1])\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                b_fc1 = bias_variable([fully_ch1])\n",
    "elif restore_flag==True:\n",
    "    with tf.device('/gpu:3'):\n",
    "        with tf.variable_scope(\"fc1\") as scope:\n",
    "            try:\n",
    "                w_fc1=tf.Variable(np.load(restore_path+'/w_fc1.npy'),name=\"fc1_W\")\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                w_fc1=tf.Variable(np.load(restore_path+'/w_fc1.npy'),name=\"fc1_W\")\n",
    "            try:\n",
    "                b_fc1=tf.Variable(np.load(restore_path+'/b_fc1.npy'),name=\"fc1_B\")\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                b_fc1=tf.Variable(np.load(restore_path+'/b_fc1.npy'),name=\"fc1_B\")\n",
    "\n",
    "        \n",
    "with tf.device('/gpu:3'): # flat conv layer \n",
    "    end_flat_conv =tf.reshape(end_conv, [-1,end_conv_col*end_conv_row*end_conv_ch])\n",
    "   \n",
    "with tf.device('/gpu:3'): # connect flat layer with fully  connnected layer \n",
    "    h_fc1 = tf.nn.relu(tf.matmul(end_flat_conv , w_fc1)+ b_fc1)\n",
    "    h_fc1 = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#connect fully connected layer \n",
    "if restore_flag==False:\n",
    "    with tf.device('/gpu:3'):\n",
    "        with tf.variable_scope(\"fc2\") as scope:\n",
    "            try:\n",
    "                w_fc2=tf.get_variable(\"fc2_W\",[fully_ch1,fully_ch2] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                w_fc2=tf.get_variable(\"fc2_W\",[fully_ch1,fully_ch2] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "            try:\n",
    "                b_fc2 = bias_variable([fully_ch2])\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                b_fc2 = bias_variable([fully_ch2])\n",
    "elif restore_flag==True:\n",
    "    with tf.device('/gpu:3'):\n",
    "        with tf.variable_scope(\"fc2\") as scope:\n",
    "            try:\n",
    "                w_fc2=tf.Variable(np.load(restore_path+'/w_fc2.npy'),name=\"fc2_W\")\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                w_fc2=tf.Variable(np.load(restore_path+'/w_fc2.npy'),name=\"fc2_W\")\n",
    "            try:\n",
    "                b_fc2=tf.Variable(np.load(restore_path+'/b_fc2.npy'),name=\"fc2_B\")\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                b_fc2=tf.Variable(np.load(restore_path+'/b_fc2.npy'),name=\"fc2_B\")\n",
    "\n",
    "with tf.device('/gpu:3'): # connect flat layer with fully  connnected layer \n",
    "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1 , w_fc2)+ b_fc2)\n",
    "    h_fc2 = tf.nn.dropout(h_fc2, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#connect fully connected layer \n",
    "if restore_flag==False:\n",
    "    with tf.device('/gpu:3'):\n",
    "        with tf.variable_scope(\"fc3\") as scope:\n",
    "            try:\n",
    "                w_fc3=tf.get_variable(\"fc3_W\",[fully_ch2,fully_ch3] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                w_fc3=tf.get_variable(\"fc3_W\",[fully_ch2,fully_ch3] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "            try:\n",
    "                b_fc3 = bias_variable([fully_ch3])\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                b_fc3 = bias_variable([fully_ch3])\n",
    "elif restore_flag==True:\n",
    "    with tf.device('/gpu:3'):\n",
    "        with tf.variable_scope(\"fc3\") as scope:\n",
    "            try:\n",
    "                w_fc3=tf.Variable(np.load(restore_path+'/w_fc3.npy'),name=\"fc3_W\")\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                w_fc3=tf.Variable(np.load(restore_path+'/w_fc3.npy'),name=\"fc3_W\")\n",
    "            try:\n",
    "                b_fc3=tf.Variable(np.load(restore_path+'/b_fc3.npy'),name=\"fc3_B\")\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                b_fc3=tf.Variable(np.load(restore_path+'/b_fc3.npy',name=\"fc3_B\"))\n",
    "\n",
    "with tf.device('/gpu:3'): # connect flat layer with fully  connnected layer \n",
    "    h_fc3 = tf.nn.relu(tf.matmul(h_fc2 , w_fc3)+ b_fc3)\n",
    "    h_fc3 = tf.nn.dropout(h_fc3, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "end_fc=h_fc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if restore_flag==False:\n",
    "    with tf.device('/gpu:3'):\n",
    "        with tf.variable_scope('fc3') as scope:\n",
    "            try:\n",
    "                w_end =tf.get_variable(\"end_W\",[fully_ch3 , n_classes ],initializer = tf.contrib.layers.xavier_initializer())\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                w_end =tf.get_variable(\"end_W\",[fully_ch3 , n_classes],initializer = tf.contrib.layers.xavier_initializer())\n",
    "            try:\n",
    "                b_end = bias_variable([n_classes])\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                b_end = bias_variable([n_classes])\n",
    "elif restore_flag==True:\n",
    "    with tf.device('/gpu:3'):\n",
    "        with tf.variable_scope(\"fc3\") as scope:\n",
    "            try:\n",
    "                w_end=tf.Variable(np.load(restore_path+'/w_end.npy'),name=\"end_W\")\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                w_end=tf.Variable(np.load(restore_path+'/w_end.npy'),name=\"end_W\")\n",
    "            try:\n",
    "                b_end=tf.Variable(np.load(restore_path+'/b_end.npy'),name=\"end_B\")\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                b_end=tf.Variable(np.load(restore_path+'/b_end.npy'),name=\"end_B\")\n",
    "\n",
    "with tf.device('/gpu:3'):  # join flat layer with fully  connnected layer \n",
    "    y_conv = tf.matmul(end_fc , w_end)+b_end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch_list(folder_path):\n",
    "    list_files=os.walk(folder_path).next()[2]\n",
    "    print list_files\n",
    "    ret_train_img_list=[]\n",
    "    ret_train_lab_list=[]\n",
    "    for i , ele in enumerate(list_files):\n",
    "\n",
    "        if 'train'  in ele and 'img'in ele:\n",
    "            ret_train_img_list.append(ele)\n",
    "        elif 'train' in ele  and  'lab' in ele:\n",
    "            ret_train_lab_list.append(ele)\n",
    "    return ret_train_img_list ,ret_train_lab_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['val_loss.npy', 'train_acc.npy', 'val_acc.npy', 'train_lab_9.npy', 'train_lab_2.npy', 'train_img_6.npy', 'train_img_0.npy', 'train_lab_7.npy', 'train_img_3.npy', 'train_img_7.npy', 'train_img_9.npy', 'train_loss.npy', 'train_img_1.npy', 'train_lab_1.npy', 'train_lab_3.npy', 'train_img_2.npy', 'train_lab_0.npy', 'train_img_8.npy', 'train_lab_5.npy', 'train_lab_6.npy', 'train_img_5.npy', 'train_lab_8.npy', 'train_lab_4.npy', 'train_img_4.npy']\n"
     ]
    }
   ],
   "source": [
    "train_images , train_labels  = get_batch_list(file_locate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_img_0.npy', 'train_img_1.npy', 'train_img_2.npy', 'train_img_3.npy', 'train_img_4.npy', 'train_img_5.npy', 'train_img_6.npy', 'train_img_7.npy', 'train_img_8.npy', 'train_img_9.npy']\n",
      "['train_lab_0.npy', 'train_lab_1.npy', 'train_lab_2.npy', 'train_lab_3.npy', 'train_lab_4.npy', 'train_lab_5.npy', 'train_lab_6.npy', 'train_lab_7.npy', 'train_lab_8.npy', 'train_lab_9.npy']\n"
     ]
    }
   ],
   "source": [
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [ atoi(c) for c in re.split('(\\d+)', text) ]\n",
    "\n",
    "\n",
    "train_images.sort(key=natural_keys)\n",
    "train_labels.sort(key = natural_keys)\n",
    "print(train_images)\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_numpy_weight( model_save_path ):\n",
    "    \n",
    "    np_w_conv1,np_w_conv2,np_w_conv3,np_w_conv4,np_w_conv5=sess.run([w_conv1,w_conv2,w_conv3,w_conv4,w_conv5])\n",
    "    np_b_conv1,np_b_conv2,np_b_conv3,np_b_conv4,np_b_conv5=sess.run([b_conv1,b_conv2,b_conv3,b_conv4,b_conv5])\n",
    "    np_w_fc1 , np_w_fc2,np_w_fc3,np_w_end=sess.run([w_fc1 , w_fc2,w_fc3 ,w_end])\n",
    "    np_b_fc1 , np_b_fc2,np_b_fc3,np_b_end=sess.run([b_fc1 , b_fc2,b_fc3,b_end])\n",
    "    \n",
    "    np_w_conv1=np.asarray(np_w_conv1)\n",
    "    np_w_conv2=np.asarray(np_w_conv2)\n",
    "    np_w_conv3=np.asarray(np_w_conv3)\n",
    "    np_w_conv4=np.asarray(np_w_conv4)\n",
    "    np_w_conv5=np.asarray(np_w_conv5)\n",
    "    \n",
    "    np_b_conv1=np.asarray(np_b_conv1)\n",
    "    np_b_conv2=np.asarray(np_b_conv2)\n",
    "    np_b_conv3=np.asarray(np_b_conv3)\n",
    "    np_b_conv4=np.asarray(np_b_conv4)\n",
    "    np_b_conv5=np.asarray(np_b_conv5)\n",
    "    \n",
    "    np_w_fc1=np.asarray(np_w_fc1)\n",
    "    np_w_fc2=np.asarray(np_w_fc2)\n",
    "    np_w_fc3=np.asarray(np_w_fc3)\n",
    "    np_w_end=np.asarray(np_w_end)\n",
    "    \n",
    "    np_b_fc1=np.asarray(np_b_fc1)\n",
    "    np_b_fc2=np.asarray(np_b_fc2)\n",
    "    np_b_fc3=np.asarray(np_b_fc3)\n",
    "    np_b_end=np.asarray(np_b_end)\n",
    "    \n",
    "    \n",
    "    np.save(model_save_path +'w_conv1' , np_w_conv1)\n",
    "    np.save(model_save_path +'w_conv2' , np_w_conv2)\n",
    "    np.save(model_save_path +'w_conv3' , np_w_conv3)\n",
    "    np.save(model_save_path +'w_conv4' , np_w_conv4)\n",
    "    np.save(model_save_path +'w_conv5' , np_w_conv5)\n",
    "    \n",
    "    np.save(model_save_path +'b_conv1' , np_b_conv1)\n",
    "    np.save(model_save_path +'b_conv2' , np_b_conv2)\n",
    "    np.save(model_save_path +'b_conv3' , np_b_conv3)\n",
    "    np.save(model_save_path +'b_conv4' , np_b_conv4)\n",
    "    np.save(model_save_path +'b_conv5' , np_b_conv5)\n",
    "\n",
    "    np.save(model_save_path +'w_fc1' , np_w_fc1)\n",
    "    np.save(model_save_path +'w_fc2' , np_w_fc2)\n",
    "    np.save(model_save_path +'w_fc3' , np_w_fc3)\n",
    "    np.save(model_save_path +'w_end' , np_w_end)\n",
    "    \n",
    "    np.save(model_save_path +'b_fc1' , np_b_fc1)\n",
    "    np.save(model_save_path +'b_fc2' , np_b_fc2)\n",
    "    np.save(model_save_path +'b_fc3' , np_b_fc3)\n",
    "    np.save(model_save_path +'b_end' , np_b_end)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aug(np_img ,crop_img_row , crop_img_col , label):\n",
    "    \"\"\"\n",
    "    np_img must 4D \n",
    "    np_img shape : n , row , col , color_ch\n",
    "    \n",
    "    \"\"\"\n",
    "    n_img,img_row,img_col,color_ch=np.shape(np_img)\n",
    "    n_ret_img = n_img*(img_row - crop_img_row) * (img_col - crop_img_col)*3\n",
    "    ret_images = np.zeros([n_ret_img ,crop_img_row , crop_img_col,color_ch])\n",
    "    len_label= np.shape(label)[1]\n",
    "    ret_labels =  np.zeros([n_ret_img  ,len_label])\n",
    "    #print \"n_augmented image size : \" , n_ret_img \n",
    "    #print \"n classes :\", len_label\n",
    "    #copy label in factor by 2014\n",
    "    count=0\n",
    "    if len(np.shape(np_img))==2:\n",
    "        np_img=np.reshape(np_img , newshape = [np.shape(np_img)[0] , img_row , img_col ,color_ch])\n",
    "        print np.shape(np_img)\n",
    "    for n  in range(0,n_img):\n",
    "        for r in range(img_row - crop_img_row):\n",
    "            for c in range(img_col - crop_img_col):\n",
    "                cropped_img = np_img[n, r:crop_img_row +r , c:crop_img_col+c ,: ]\n",
    "                ret_images[count*3,:,:,:]=cropped_img  \n",
    "                ret_images[(count*3+1) , :,:,:] =np.fliplr(cropped_img )\n",
    "                ret_images[(count*3+2) , :,:,:] =np.flipud(cropped_img )\n",
    "                \n",
    "                ret_labels[count*3 , : ] = label[n,:]\n",
    "                ret_labels[count*3+1 , : ] = label[n,:]                \n",
    "                ret_labels[count*3+2 , : ] = label[n,:]\n",
    "                count+=1\n",
    "    return ret_images ,ret_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-21-58dfae6e22df>:19 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-58dfae6e22df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:3'):\n",
    "#sm_conv= tf.nn.softmax(y_conv)\n",
    "    #cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "    start_time = time.time()\n",
    "\n",
    "    regular=0.01*(tf.reduce_sum(tf.square(y_conv)))\n",
    "    pred=tf.nn.softmax(y_conv)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits( y_conv, y_))\n",
    "with tf.device('/gpu:3'):\n",
    "    cost = cost+regular\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(cost) #1e-4\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            correct_prediction = tf.equal(tf.argmax(y_conv,1) ,tf.argmax(y_,1))\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction , \"float\")) \n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.initialize_all_variables())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_count=0\n",
    "max_acc=0\n",
    "if divide_flag ==True:\n",
    "    n_batch =len(train_images)\n",
    "    batch_count=0\n",
    "show_Exception_flag=True\n",
    "val_acc_list=[]\n",
    "val_loss_list=[]\n",
    "train_acc_list=[]\n",
    "train_loss_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is recorded at :37\n"
     ]
    }
   ],
   "source": [
    "#dirname = '/home/ncc/notebook/mammo/result/'\n",
    "\n",
    "dirname='../result/'    \n",
    "count=0\n",
    "while(True):\n",
    "    if not os.path.isdir(dirname):\n",
    "        os.mkdir(dirname)\n",
    "        break\n",
    "    elif not os.path.isdir(dirname + str(count)):\n",
    "        dirname=dirname+str(count)\n",
    "        os.mkdir(dirname)\n",
    "        break\n",
    "    else:\n",
    "        count+=1\n",
    "print 'it is recorded at :'+str(count)\n",
    "f=open(dirname+\"/log.txt\",'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_aug(img ,lab, crop_row, crop_col  , color_ch , n_classes):\n",
    "    test_aug=np.zeros([10,img_row , img_col , color_ch])\n",
    "    test_lab=np.zeros([10,n_classes])\n",
    "    for i in range(10):\n",
    "        test_lab[i,:] = lab\n",
    "    ori_row , ori_col , color_ch = np.shape(img)\n",
    "    test_aug[0]=img[:crop_row , :crop_col, :]\n",
    "    test_aug[1]=img[-crop_row :, :crop_col, :]\n",
    "    test_aug[2]=img[:crop_row , -crop_col:, :]\n",
    "    test_aug[3]=img[-crop_row: , -crop_col:, :]\n",
    "    test_aug[4]=img[(ori_row/2)-(crop_row/2) :(ori_row/2)+(crop_row/2),\\\n",
    "                    (ori_col/2)-(crop_col/2) :(ori_col/2)+(crop_col/2), :]\n",
    "    img=np.fliplr(img)\n",
    "    test_aug[5]=img[:crop_row , :crop_row, :]\n",
    "    test_aug[6]=img[-crop_row :, :crop_row, :]\n",
    "    test_aug[7]=img[:crop_row , -crop_row:, :]\n",
    "    test_aug[8]=img[-crop_row: , -crop_row:, :]\n",
    "    test_aug[4]=img[(ori_row/2)-(crop_row/2) :(ori_row/2)+(crop_row/2),\\\n",
    "                    (ori_col/2)-(crop_col/2) :(ori_col/2)+(crop_col/2), :]\n",
    "    \n",
    "    return test_aug, test_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to use uninitialized value layer1/W1\n",
      "\t [[Node: layer1/W1/read = Identity[T=DT_FLOAT, _class=[\"loc:@layer1/W1\"], _device=\"/job:localhost/replica:0/task:0/gpu:3\"](layer1/W1)]]\n",
      "\t [[Node: add_9/_9 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:3\", send_device_incarnation=1, tensor_name=\"edge_221_add_9\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
      "\n",
      "Caused by op u'layer1/W1/read', defined at:\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n",
      "    \"__main__\", fname, loader, pkg_name)\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n",
      "    exec code in run_globals\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 474, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 887, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-9928c59fadbb>\", line 6, in <module>\n",
      "    w_conv1 = tf.get_variable(\"W1\",[weight_row,weight_col,in_ch,out_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1024, in get_variable\n",
      "    custom_getter=custom_getter)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 850, in get_variable\n",
      "    custom_getter=custom_getter)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 346, in get_variable\n",
      "    validate_shape=validate_shape)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 331, in _true_getter\n",
      "    caching_device=caching_device, validate_shape=validate_shape)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 677, in _get_single_variable\n",
      "    expected_shape=shape)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 224, in __init__\n",
      "    expected_shape=expected_shape)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 370, in _init_from_args\n",
      "    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 1424, in identity\n",
      "    result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n",
      "    op_def=op_def)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n",
      "    original_op=self._default_original_op, op_def=op_def)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n",
      "    self._traceback = _extract_stack()\n",
      "\n",
      "FailedPreconditionError (see above for traceback): Attempting to use uninitialized value layer1/W1\n",
      "\t [[Node: layer1/W1/read = Identity[T=DT_FLOAT, _class=[\"loc:@layer1/W1\"], _device=\"/job:localhost/replica:0/task:0/gpu:3\"](layer1/W1)]]\n",
      "\t [[Node: add_9/_9 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:3\", send_device_incarnation=1, tensor_name=\"edge_221_add_9\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
      "\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value layer1/W1\n\t [[Node: layer1/W1/read = Identity[T=DT_FLOAT, _class=[\"loc:@layer1/W1\"], _device=\"/job:localhost/replica:0/task:0/gpu:3\"](layer1/W1)]]\n\t [[Node: accuracy/accuracy/Mean/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:3\", send_device_incarnation=1, tensor_name=\"edge_46_accuracy/accuracy/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op u'layer1/W1/read', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-9928c59fadbb>\", line 6, in <module>\n    w_conv1 = tf.get_variable(\"W1\",[weight_row,weight_col,in_ch,out_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1024, in get_variable\n    custom_getter=custom_getter)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 850, in get_variable\n    custom_getter=custom_getter)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 346, in get_variable\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 331, in _true_getter\n    caching_device=caching_device, validate_shape=validate_shape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 677, in _get_single_variable\n    expected_shape=shape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 224, in __init__\n    expected_shape=expected_shape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 370, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 1424, in identity\n    result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value layer1/W1\n\t [[Node: layer1/W1/read = Identity[T=DT_FLOAT, _class=[\"loc:@layer1/W1\"], _device=\"/job:localhost/replica:0/task:0/gpu:3\"](layer1/W1)]]\n\t [[Node: accuracy/accuracy/Mean/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:3\", send_device_incarnation=1, tensor_name=\"edge_46_accuracy/accuracy/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-6636af2ab915>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_divide\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                     \u001b[0;31m# j*batch_size :(j+1)*batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                     \u001b[0mval_accuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mval_img\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mval_lab\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                     \u001b[0mlist_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mlist_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mval_accuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mval_img\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mval_lab\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m:\u001b[0m  \u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value layer1/W1\n\t [[Node: layer1/W1/read = Identity[T=DT_FLOAT, _class=[\"loc:@layer1/W1\"], _device=\"/job:localhost/replica:0/task:0/gpu:3\"](layer1/W1)]]\n\t [[Node: accuracy/accuracy/Mean/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:3\", send_device_incarnation=1, tensor_name=\"edge_46_accuracy/accuracy/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op u'layer1/W1/read', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-9928c59fadbb>\", line 6, in <module>\n    w_conv1 = tf.get_variable(\"W1\",[weight_row,weight_col,in_ch,out_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1024, in get_variable\n    custom_getter=custom_getter)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 850, in get_variable\n    custom_getter=custom_getter)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 346, in get_variable\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 331, in _true_getter\n    caching_device=caching_device, validate_shape=validate_shape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 677, in _get_single_variable\n    expected_shape=shape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 224, in __init__\n    expected_shape=expected_shape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 370, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 1424, in identity\n    result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value layer1/W1\n\t [[Node: layer1/W1/read = Identity[T=DT_FLOAT, _class=[\"loc:@layer1/W1\"], _device=\"/job:localhost/replica:0/task:0/gpu:3\"](layer1/W1)]]\n\t [[Node: accuracy/accuracy/Mean/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:3\", send_device_incarnation=1, tensor_name=\"edge_46_accuracy/accuracy/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:3'):\n",
    "    train_ind=0\n",
    "    for i in range(iterate):    \n",
    "        if divide_flag ==True:\n",
    "            if batch_count >= n_batch:\n",
    "                batch_count =0\n",
    "            train_img =np.load(file_locate+train_images[batch_count])\n",
    "            train_lab =np.load(file_locate+train_labels[batch_count])\n",
    "        batch_xs , batch_ys = next_batch(batch_size, train_img , train_lab)\n",
    "        #batch_val_xs  , batch_val_ys = next_batch(20 , val_img , val_lab)\n",
    "        if i%100 ==0: # in here add to validation \n",
    "\n",
    "            try:\n",
    "                if aug_flag==True:\n",
    "                    print 'aug'\n",
    "                    aug_val_acc_list=[]\n",
    "                    aug_val_loss_list=[]\n",
    "                    aug_test_acc_list=[]\n",
    "                    aug_test_loss_list=[]\n",
    "\n",
    "                    for k in range(len(val_img)):\n",
    "                        aug_val_img , aug_val_lab=test_aug(val_img[k] , val_lab[k] , 64,64, 1 , 2)\n",
    "                        aug_test_img , aug_test_lab=test_aug(test_img[k] , test_lab[k] , 64,64, 1 , 2)                    \n",
    "                        val_accuracy ,val_loss = sess.run( [accuracy,cost] , feed_dict={x:aug_val_img , y_:aug_val_lab , keep_prob: 1.0})        \n",
    "                        test_accuracy,test_loss= sess.run([accuracy,cost]  , feed_dict={x:aug_test_img , y_:aug_test_lab , keep_prob: 1.0})\n",
    "                        aug_val_acc_list.append(val_accuracy);aug_val_loss_list.append(val_loss);\n",
    "                        aug_test_acc_list.append(test_accuracy);aug_test_loss_list.append(test_loss);\n",
    "                    val_accuracy=np.mean(np.asarray(aug_val_acc_list));val_loss=np.mean(np.asarray(aug_val_loss_list))\n",
    "                    test_accuracy=np.mean(np.asarray(aug_test_acc_list));test_loss=np.mean(np.asarray(aug_test_loss_list))\n",
    "                    print 'a'\n",
    "                    print i\n",
    "                    if i==0:\n",
    "                        print 'a'\n",
    "                        train_accuracy=0.0\n",
    "                        train_loss=0.0\n",
    "                else:\n",
    "                    val_accuracy ,val_loss = sess.run( [accuracy,cost] , feed_dict={x:val_img , y_:val_lab , keep_prob: 1.0})        \n",
    "                    train_accuracy ,train_loss= sess.run([accuracy,cost] , feed_dict={x:batch_xs , y_:batch_ys , keep_prob: 1.0})        \n",
    "                    test_accuracy,test_loss= sess.run([accuracy,cost]  , feed_dict={x:test_img , y_:test_lab , keep_prob: 1.0})\n",
    "\n",
    "\n",
    "                if (val_accuracy+test_accuracy)/2 > max_acc:\n",
    "                    print 'model_was_saved'\n",
    "                    if save_flag == True:\n",
    "                        save_numpy_weight(model_save_path)\n",
    "                    max_acc=(val_accuracy+test_accuracy)/2\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                if show_Exception_flag:\n",
    "                    print str(e)\n",
    "                    show_Exception_flag=False\n",
    "\n",
    "                list_acc=[]\n",
    "                list_loss=[]\n",
    "                n_divide=len(val_img)/batch_size\n",
    "                j=0\n",
    "\n",
    "                #validation \n",
    "                for j in range(n_divide):\n",
    "                    # j*batch_size :(j+1)*batch_size\n",
    "                    val_accuracy,val_loss = sess.run([accuracy ,cost], feed_dict={x:val_img[ j*batch_size :(j+1)*batch_size] , y_:val_lab[ j*batch_size :(j+1)*batch_size ] , keep_prob: 1.0})        \n",
    "                    list_acc.append(float(val_accuracy));list_loss.append(float(val_loss))\n",
    "                val_accuracy,val_loss = sess.run([accuracy ,cost], feed_dict={x:val_img[ j*batch_size :] , y_:val_lab[ j*batch_size :  ] , keep_prob: 1.0})         \n",
    "                list_acc=np.asarray(list_acc);list_loss= np.asarray(list_loss);\n",
    "                val_accuracy=np.mean(list_acc);val_loss = np.mean(list_loss); \n",
    "                val_acc_list.append(val_accuracy)\n",
    "                val_loss_list.append(val_loss)\n",
    "\n",
    "                if (val_accuracy+test_accuracy)/2 > max_acc:\n",
    "                    print 'model_was_saved'\n",
    "                    max_acc=(val_accuracy+test_accuracy)/2\n",
    "                    if save_flag == True:\n",
    "                        print 'model_was_upgraded'\n",
    "                        save_numpy_weight(model_save_path)\n",
    "\n",
    "\n",
    "                #testing    \n",
    "                test_list_acc=[]\n",
    "                test_list_loss=[]        \n",
    "                for j in range(n_divide):    \n",
    "                    # j*batch_size :(j+1)*batch_size\n",
    "                    test_accuracy,test_loss = sess.run([accuracy ,cost], feed_dict={x:test_img[ j*batch_size :(j+1)*batch_size] , y_:test_lab[ j*batch_size :(j+1)*batch_size ] , keep_prob: 1.0})        \n",
    "                    test_list_acc.append(float(test_accuracy));test_list_loss.append(float(test_loss))\n",
    "                #right above code have to modify\n",
    "                test_accuracy,test_loss = sess.run([accuracy ,cost], feed_dict={x:val_img[ j*batch_size :] , y_:val_lab[ j*batch_size :  ] , keep_prob: 1.0})         \n",
    "                test_list_acc.append(test_accuracy);test_list_loss.append(test_loss)\n",
    "                #result = sess.run(sm_conv , feed_dict = {x:val_img , y_:batch_ys , keep_prob :1.0})\n",
    "                train_accuracy ,train_loss = sess.run( [accuracy ,cost], feed_dict={x:batch_xs , y_:batch_ys , keep_prob: 1.0})        \n",
    "\n",
    "            ###record val_acc , loss  and train acc ,and loss\n",
    "            val_acc_list.append(val_accuracy);val_loss_list.append(val_loss)\n",
    "            train_acc_list.append(train_accuracy);train_loss_list.append(train_loss)                \n",
    "            ###record train_acc,loss to file\n",
    "            print(\"step %d , training  accuracy %g\" %(i,train_accuracy))\n",
    "            print(\"step %d , loss : %g\" %(i,train_loss))\n",
    "            train_str = 'step:\\t'+str(i)+'\\tval_loss:\\t'+str(train_loss) +'\\tval accuracy:\\t'+str(train_accuracy)+'\\n'\n",
    "            ###record val_acc,loss to file\n",
    "            print(\"step %d , validation  accuracy %g\" %(i,val_accuracy))\n",
    "            print(\"step %d , validation loss : %g\" %(i,val_loss))\n",
    "            val_str = 'step:\\t'+str(i)+'\\tval_loss:\\t'+str(val_loss) +'\\tval accuracy:\\t'+str(val_accuracy)+'\\n'\n",
    "            ###record val_acc,loss to file\n",
    "            print(\"step %d , test  accuracy %g\" %(i,test_accuracy))\n",
    "            print(\"step %d , test loss : %g\" %(i,test_loss))           \n",
    "\n",
    "            f.write(val_str)\n",
    "            f.write(train_str)\n",
    "            if divide_flag ==True:\n",
    "                batch_count+=1\n",
    "            ####training####\n",
    "            if aug_flag==True:\n",
    "                if train_ind == len(train_img):\n",
    "                    train_ind=0\n",
    "                aug_imgs , aug_labs=aug(train_img[train_ind:train_ind+1] ,64,64,train_lab[train_ind:train_ind+1])\n",
    "                try:\n",
    "                    sess.run(train_step ,feed_dict={x:aug_imgs , y_:aug_labs , keep_prob : 0.7})\n",
    "                    train_ind+=1\n",
    "                except Exception as e:\n",
    "\n",
    "                    if show_Exception_flag:\n",
    "                        print str(e)\n",
    "                        show_Exception_flag=False\n",
    "                    acc_=[]\n",
    "                    loss_=[]\n",
    "                    divide=len(aug_imgs)/batch_size\n",
    "                    for d in range(divide):\n",
    "                        _,acc,loss=sess.run([train_step,accuracy,cost] ,feed_dict={x:aug_imgs[d*batch_size:(d+1)*batch_size] ,\\\n",
    "                                                        y_:aug_labs[d*batch_size:(d+1)*batch_size] , keep_prob : 0.7})\n",
    "                        acc_.append(acc);loss_.append(loss);\n",
    "                    _,acc,loss=sess.run([train_step,accuracy,cost],feed_dict={x:aug_imgs[d*batch_size:] , y_:aug_labs[d*batch_size:] , keep_prob : 0.7})\n",
    "                    acc_.append(acc);loss_.append(loss)\n",
    "                    train_accuracy=np.mean(np.asarray(acc_))\n",
    "                    train_loss=np.mean(np.asarray(loss_))\n",
    "                    train_ind+=1\n",
    "\n",
    "\n",
    "\n",
    "            else:\n",
    "                sess.run(train_step ,feed_dict={x:batch_xs , y_:batch_ys , keep_prob : 0.7})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    np.save(model_save_path+'val_acc',np.asarray(val_acc_list))\n",
    "    np.save(model_save_path+'val_loss',np.asarray(val_loss_list))\n",
    "    np.save(model_save_path+'train_acc',np.asarray(train_acc_list))\n",
    "    np.save(model_save_path+'train_loss',np.asarray(train_loss_list))\n",
    "    softmax_=sess.run( pred , feed_dict={x:test_img  ,y_:test_lab, keep_prob: 1.0})\n",
    "    test_accuracy,test_loss= sess.run([accuracy,cost]  , feed_dict={x:test_img , y_:test_lab , keep_prob: 1.0})\n",
    "    print(\"--- Training Time : %s ---\" % (time.time() - start_time))\n",
    "    train_time=\"--- Training Time : ---:\\t\" +str(time.time() - start_time)\n",
    "    f.write(train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print softmax_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_img=np.load('/home/seongjung/save_numpy/1.npy')\n",
    "print np.shape(test_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    softmax_=sess.run( accuracy , feed_dict={x:test_img  , keep_prob: 1.0})\n",
    "    test_accuracy = sess.run( accuracy , feed_dict={x:test_img , y_:test_lab , keep_prob: 1.0})        \n",
    "    test_loss = sess.run(cost , feed_dict = {x:test_img , y_: test_lab , keep_prob: 1.0})\n",
    "\n",
    "    #result = sess.run(sm_conv , feed_dict = {x:test_img , y_:batch_ys , keep_prob :1.0})\n",
    "    print(\"step %d , testidation  accuracy %g\" %(i,test_accuracy))\n",
    "    print(\"step %d , testidation loss : %g\" %(i,test_loss))\n",
    "    test_str = 'step:\\t'+str(i)+'\\ttest_loss:\\t'+str(test_loss) +'\\ttest accuracy:\\t'+str(test_accuracy)+'\\n'\n",
    "\n",
    "    f.write(test_str)\n",
    "except :\n",
    "    list_acc=[]\n",
    "    list_loss=[]\n",
    "    n_divide=len(test_img)/batch_size\n",
    "    for j in range(n_divide):\n",
    "\n",
    "        # j*batch_size :(j+1)*batch_size\n",
    "        test_accuracy,test_loss = sess.run([accuracy ,cost], feed_dict={x:test_img[ j*batch_size :(j+1)*batch_size] , y_:test_lab[ j*batch_size :(j+1)*batch_size ] , keep_prob: 1.0})        \n",
    "        list_acc.append(float(test_accuracy))\n",
    "        list_loss.append(float(test_loss))\n",
    "    test_accuracy , test_loss=sess.run([accuracy,cost] , feed_dict={x:test_img[(j+1)*batch_size : ] , y_:test_lab[(j+1)*(batch_size) : ] , keep_prob : 1.0})\n",
    "    #right above code have to modify\n",
    "\n",
    "    list_acc.append(test_accuracy)\n",
    "    list_loss.append(test_loss)\n",
    "    list_acc=np.asarray(list_acc)\n",
    "    list_loss= np.asarray(list_loss)\n",
    "\n",
    "    test_accuracy=np.mean(list_acc)\n",
    "    test_loss = np.mean(list_loss)\n",
    "\n",
    "    #result = sess.run(sm_conv , feed_dict = {x:test_img , y_:batch_ys , keep_prob :1.0})\n",
    "    print(\"step %d , testidation  accuracy %g\" %(i,test_accuracy))\n",
    "    print(\"step %d , testidation loss : %g\" %(i,test_loss))\n",
    "    test_str = 'step:\\t'+str(i)+'\\ttest_loss:\\t'+str(test_loss) +'\\ttest accuracy:\\t'+str(test_accuracy)+'\\n'\n",
    "\n",
    "    f.write(test_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(train_img[8])\n",
    "train_lab[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(test_img[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
